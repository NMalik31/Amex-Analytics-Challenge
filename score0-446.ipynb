{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12371897,"sourceType":"datasetVersion","datasetId":7800477}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:44:50.881925Z","iopub.execute_input":"2025-07-04T18:44:50.882217Z","iopub.status.idle":"2025-07-04T18:44:51.277807Z","shell.execute_reply.started":"2025-07-04T18:44:50.882194Z","shell.execute_reply":"2025-07-04T18:44:51.277024Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/amex-challenge/test_data.parquet\n/kaggle/input/amex-challenge/add_event.parquet\n/kaggle/input/amex-challenge/685404e30cfdb_submission_template.csv\n/kaggle/input/amex-challenge/data_dictionary.csv\n/kaggle/input/amex-challenge/offer_metadata.parquet\n/kaggle/input/amex-challenge/add_trans.parquet\n/kaggle/input/amex-challenge/train_data.parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# STEP 0: Imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold, KFold\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:44:51.278986Z","iopub.execute_input":"2025-07-04T18:44:51.279440Z","iopub.status.idle":"2025-07-04T18:44:56.207681Z","shell.execute_reply.started":"2025-07-04T18:44:51.279419Z","shell.execute_reply":"2025-07-04T18:44:56.207095Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# STEP 1: Load Data\nbase = '/kaggle/input/amex-challenge/'\ntrain = pd.read_parquet(base + 'train_data.parquet')\ntest = pd.read_parquet(base + 'test_data.parquet')\nevents = pd.read_parquet(base + 'add_event.parquet')\ntrans = pd.read_parquet(base + 'add_trans.parquet')\noffers = pd.read_parquet(base + 'offer_metadata.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:44:56.208349Z","iopub.execute_input":"2025-07-04T18:44:56.208781Z","iopub.status.idle":"2025-07-04T18:45:37.017937Z","shell.execute_reply.started":"2025-07-04T18:44:56.208754Z","shell.execute_reply":"2025-07-04T18:45:37.017105Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Standardize IDs\nfor df in [train, test, events, trans]:\n    df['id2'] = df['id2'].astype(str)\n    if 'id3' in df.columns:\n        df['id3'] = df['id3'].astype(str)\noffers['id3'] = offers['id3'].astype(str)\n\n# Merge offer metadata\ntrain = train.merge(offers, on='id3', how='left')\ntest = test.merge(offers, on='id3', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:37.018967Z","iopub.execute_input":"2025-07-04T18:45:37.019239Z","iopub.status.idle":"2025-07-04T18:46:03.835902Z","shell.execute_reply.started":"2025-07-04T18:45:37.019219Z","shell.execute_reply":"2025-07-04T18:46:03.835325Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Aggregate transaction features\nagg_trans = trans.groupby('id2')['f367'].agg(['sum', 'mean', 'count']).reset_index()\nagg_trans.columns = ['id2', 'total_spend', 'avg_spend', 'txn_count']\ntrain = train.merge(agg_trans, on='id2', how='left')\ntest = test.merge(agg_trans, on='id2', how='left')\n# Event interaction rate\nevents['click_flag'] = events['id7'].notnull().astype(int)\nclick_rate = events.groupby('id2')['click_flag'].mean().reset_index(name='click_rate')\ntrain = train.merge(click_rate, on='id2', how='left')\ntest = test.merge(click_rate, on='id2', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:46:03.837952Z","iopub.execute_input":"2025-07-04T18:46:03.838195Z","iopub.status.idle":"2025-07-04T18:46:31.765590Z","shell.execute_reply.started":"2025-07-04T18:46:03.838176Z","shell.execute_reply":"2025-07-04T18:46:31.764959Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Temporal features\nfor df in [train, test]:\n    df['id4'] = pd.to_datetime(df['id4'], errors='coerce')\n    df['day'] = df['id4'].dt.dayofweek\n    df['hour'] = df['id4'].dt.hour\n    df['recency'] = (df['id4'].max() - df['id4']).dt.days\n    df['id5'] = df['id4'].dt.strftime('%m/%d/%y')  # MM/DD/YY for submission\n# TF-IDF on offer body (f378)\ntfidf = TfidfVectorizer(max_features=50)\ntfidf_train = tfidf.fit_transform(train['f378'].fillna(''))\ntfidf_test = tfidf.transform(test['f378'].fillna(''))\ntfidf_train_df = pd.DataFrame(tfidf_train.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf_train.shape[1])])\ntfidf_test_df = pd.DataFrame(tfidf_test.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf_test.shape[1])])\ntrain = pd.concat([train.reset_index(drop=True), tfidf_train_df], axis=1)\ntest = pd.concat([test.reset_index(drop=True), tfidf_test_df], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:46:31.766300Z","iopub.execute_input":"2025-07-04T18:46:31.766489Z","iopub.status.idle":"2025-07-04T18:47:04.311448Z","shell.execute_reply.started":"2025-07-04T18:46:31.766473Z","shell.execute_reply":"2025-07-04T18:47:04.310743Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import KFold\n\n# Convert y to numeric and drop any rows where conversion fails\ntrain['y'] = pd.to_numeric(train['y'], errors='coerce')\ntrain = train.dropna(subset=['y'])\ntrain['y'] = train['y'].astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:47:04.312292Z","iopub.execute_input":"2025-07-04T18:47:04.312516Z","iopub.status.idle":"2025-07-04T18:47:17.172113Z","shell.execute_reply.started":"2025-07-04T18:47:04.312498Z","shell.execute_reply":"2025-07-04T18:47:17.171380Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\ntrain['id3_te'] = 0.0  # Use float for encoding\n\nfor tr_idx, val_idx in kf.split(train):\n    means = train.iloc[tr_idx].groupby('id3')['y'].mean()\n    train.loc[val_idx, 'id3_te'] = train.loc[val_idx, 'id3'].map(means)\n\n# For test set, use global means from full train\ntest['id3_te'] = test['id3'].map(train.groupby('id3')['y'].mean())\n\n# Fill missing encodings with global mean\ntrain['id3_te'].fillna(train['y'].mean(), inplace=True)\ntest['id3_te'].fillna(train['y'].mean(), inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:47:17.172962Z","iopub.execute_input":"2025-07-04T18:47:17.173237Z","iopub.status.idle":"2025-07-04T18:47:32.422834Z","shell.execute_reply.started":"2025-07-04T18:47:17.173216Z","shell.execute_reply":"2025-07-04T18:47:32.421973Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_137/555259270.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train['id3_te'].fillna(train['y'].mean(), inplace=True)\n/tmp/ipykernel_137/555259270.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test['id3_te'].fillna(train['y'].mean(), inplace=True)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"for df in [train, test]:\n    # Spend × click rate\n    df['spend_click'] = df['total_spend'] * df['click_rate']\n    # Offer popularity: number of customers per offer\n    df['offer_pop'] = df.groupby('id3')['id2'].transform('count')\n    # Customer frequency: number of offers per customer\n    df['cust_freq'] = df.groupby('id2')['id3'].transform('count')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:47:32.423804Z","iopub.execute_input":"2025-07-04T18:47:32.424679Z","iopub.status.idle":"2025-07-04T18:47:33.051175Z","shell.execute_reply.started":"2025-07-04T18:47:32.424656Z","shell.execute_reply":"2025-07-04T18:47:33.050603Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Exclude non-feature columns\nexclude = ['id1', 'id2', 'id3', 'id4', 'id5', 'id12', 'id13', 'f378', 'start', 'end']\nfeatures = [col for col in train.columns if col not in exclude + ['y']]\n\n# Ensure all features are numeric\nfor col in features:\n    train[col] = pd.to_numeric(train[col], errors='coerce')\n    test[col] = pd.to_numeric(test[col], errors='coerce')\n\ntrain.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)\n\nX = train[features]\ny = train['y']\nX_test = test[features]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:47:33.052040Z","iopub.execute_input":"2025-07-04T18:47:33.052572Z","iopub.status.idle":"2025-07-04T18:49:10.156613Z","shell.execute_reply.started":"2025-07-04T18:47:33.052540Z","shell.execute_reply":"2025-07-04T18:49:10.155959Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nimport numpy as np\nimport gc\n\ngkf = GroupKFold(n_splits=5)\npreds = np.zeros(len(test))\ntrain['pred'] = 0\n\nparams = {\n    'objective': 'lambdarank',\n    'metric': 'map',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.05,\n    'num_leaves': 64,\n    'min_data_in_leaf': 50,\n    'max_depth': 8,\n    'verbosity': -1,\n    'device': 'gpu'\n}\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y, groups=train['id2'])):\n    X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx].astype(float)\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx].astype(float)\n    group_tr = train.iloc[tr_idx].groupby('id2').size().values\n    group_val = train.iloc[val_idx].groupby('id2').size().values\n\n    dtrain = lgb.Dataset(X_tr, y_tr, group=group_tr)\n    dval = lgb.Dataset(X_val, y_val, group=group_val)\n    model = lgb.train(params, dtrain, valid_sets=[dval], num_boost_round=1000,\n                      callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)])\n    train.loc[val_idx, 'pred'] = model.predict(X_val)\n    preds += model.predict(X_test) / gkf.get_n_splits()\n    gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:49:10.157487Z","iopub.execute_input":"2025-07-04T18:49:10.157774Z","iopub.status.idle":"2025-07-04T18:51:59.210928Z","shell.execute_reply.started":"2025-07-04T18:49:10.157747Z","shell.execute_reply":"2025-07-04T18:51:59.210117Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_137/1039245366.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train['pred'] = 0\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\n[100]\tvalid_0's map@1: 0.927927\tvalid_0's map@2: 0.927095\tvalid_0's map@3: 0.929144\tvalid_0's map@4: 0.930311\tvalid_0's map@5: 0.931352\nEarly stopping, best iteration is:\n[108]\tvalid_0's map@1: 0.928142\tvalid_0's map@2: 0.927175\tvalid_0's map@3: 0.929341\tvalid_0's map@4: 0.930594\tvalid_0's map@5: 0.93164\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_137/1039245366.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-2.54738428 -3.54769246 -3.70538835 ... -2.8381854  -2.26091068\n -2.37773633]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  train.loc[val_idx, 'pred'] = model.predict(X_val)\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\n[100]\tvalid_0's map@1: 0.936412\tvalid_0's map@2: 0.933351\tvalid_0's map@3: 0.934178\tvalid_0's map@4: 0.934584\tvalid_0's map@5: 0.935279\nEarly stopping, best iteration is:\n[97]\tvalid_0's map@1: 0.93609\tvalid_0's map@2: 0.933512\tvalid_0's map@3: 0.934225\tvalid_0's map@4: 0.93462\tvalid_0's map@5: 0.935226\nTraining until validation scores don't improve for 50 rounds\n[100]\tvalid_0's map@1: 0.934801\tvalid_0's map@2: 0.933002\tvalid_0's map@3: 0.934243\tvalid_0's map@4: 0.935456\tvalid_0's map@5: 0.935885\n[200]\tvalid_0's map@1: 0.935553\tvalid_0's map@2: 0.933405\tvalid_0's map@3: 0.934661\tvalid_0's map@4: 0.935572\tvalid_0's map@5: 0.935944\nEarly stopping, best iteration is:\n[158]\tvalid_0's map@1: 0.935768\tvalid_0's map@2: 0.933566\tvalid_0's map@3: 0.934697\tvalid_0's map@4: 0.935835\tvalid_0's map@5: 0.936317\nTraining until validation scores don't improve for 50 rounds\n[100]\tvalid_0's map@1: 0.931257\tvalid_0's map@2: 0.931579\tvalid_0's map@3: 0.932593\tvalid_0's map@4: 0.933537\tvalid_0's map@5: 0.934136\nEarly stopping, best iteration is:\n[128]\tvalid_0's map@1: 0.933512\tvalid_0's map@2: 0.932223\tvalid_0's map@3: 0.933333\tvalid_0's map@4: 0.934209\tvalid_0's map@5: 0.934621\nTraining until validation scores don't improve for 50 rounds\n[100]\tvalid_0's map@1: 0.929646\tvalid_0's map@2: 0.928491\tvalid_0's map@3: 0.928837\tvalid_0's map@4: 0.929762\tvalid_0's map@5: 0.930241\nEarly stopping, best iteration is:\n[103]\tvalid_0's map@1: 0.93029\tvalid_0's map@2: 0.928652\tvalid_0's map@3: 0.929219\tvalid_0's map@4: 0.930019\tvalid_0's map@5: 0.930455\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Prepare submission DataFrame\nsubmission = test[['id1', 'id2', 'id3', 'id5']].copy()\nsubmission['pred'] = preds\n\n# Normalize predictions per group (id2) for ranking\nsubmission['pred'] = submission.groupby('id2')['pred'].transform(\n    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-9)\n)\n\n# Ensure id5 is in MM/DD/YY format (if not already)\nsubmission['id5'] = pd.to_datetime(submission['id5'], errors='coerce').dt.strftime('%m/%d/%y')\n\n# Save CSV\nsubmission.to_csv('final_submission.csv', index=False)\nprint(\"✅ Final submission file saved with normalized prediction scores.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:53:30.171971Z","iopub.execute_input":"2025-07-04T18:53:30.172666Z","iopub.status.idle":"2025-07-04T18:53:37.578494Z","shell.execute_reply.started":"2025-07-04T18:53:30.172640Z","shell.execute_reply":"2025-07-04T18:53:37.577811Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_137/3962432019.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  submission['id5'] = pd.to_datetime(submission['id5'], errors='coerce').dt.strftime('%m/%d/%y')\n","output_type":"stream"},{"name":"stdout","text":"✅ Final submission file saved with normalized prediction scores.\n","output_type":"stream"}],"execution_count":12}]}